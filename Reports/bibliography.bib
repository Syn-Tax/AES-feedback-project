
@inproceedings{alikaniotisAutomaticTextScoring2016,
  title = {Automatic {{Text Scoring Using Neural Networks}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Alikaniotis, Dimitrios and Yannakoudakis, Helen and Rei, Marek},
  year = {2016},
  eprint = {1606.04289},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {715--725},
  doi = {10.18653/v1/P16-1068},
  abstract = {Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,I.2.7,I.5.1},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/KQEPL8HC/Alikaniotis et al. - 2016 - Automatic Text Scoring Using Neural Networks.pdf}
}

@article{basuPowergradingClusteringApproach2013,
  title = {Powergrading: A {{Clustering Approach}} to {{Amplify Human Effort}} for {{Short Answer Grading}}},
  shorttitle = {Powergrading},
  author = {Basu, Sumit and Jacobs, Chuck and Vanderwende, Lucy},
  year = {2013},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {1},
  pages = {391--402},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00236},
  abstract = {We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as ``powergrading.'' We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small ``budget'' of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/TLCSDHUD/Basu et al. - 2013 - Powergrading a Clustering Approach to Amplify Hum.pdf}
}

@article{beltagyLongformerLongDocumentTransformer2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = {2020},
  month = dec,
  journal = {arXiv:2004.05150 [cs]},
  eprint = {2004.05150},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/TZ2BCBVA/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/Q6KYBE8Z/2004.html}
}

@inproceedings{davidGeneticAlgorithmsEvolving2014,
  title = {Genetic {{Algorithms}} for {{Evolving Deep Neural Networks}}},
  booktitle = {Proceedings of the {{Companion Publication}} of the 2014 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {David, Eli and Greental, Iddo},
  year = {2014},
  month = jul,
  eprint = {1711.07655},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1451--1452},
  doi = {10.1145/2598394.2602287},
  abstract = {In recent years, deep learning methods applying unsupervised learning to train deep layers of neural networks have achieved remarkable results in numerous fields. In the past, many genetic algorithms based methods have been successfully applied to training neural networks. In this paper, we extend previous work and propose a GA-assisted method for deep learning. Our experimental results indicate that this GA-assisted approach improves the performance of a deep autoencoder, producing a sparser neural network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/7ENM7R4X/David and Greental - 2014 - Genetic Algorithms for Evolving Deep Neural Networ.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/NA8CXRWE/1711.html}
}

@article{devlinBERTPreTrainingDeep2019,
  title = {{{BERT}}: {{Pre-Training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,model}
}

@article{dzikovskaEffectiveTutorialFeedback,
  title = {Towards {{Effective Tutorial Feedback}} for {{Explanation Questions}}: {{A Dataset}} and {{Baselines}}},
  author = {Dzikovska, Myroslava O and Nielsen, Rodney D and Brew, Chris},
  pages = {11},
  abstract = {We propose a new shared task on grading student answers with the goal of enabling welltargeted and flexible feedback in a tutorial dialogue setting. We provide an annotated corpus designed for the purpose, a precise specification for a prediction task and an associated evaluation methodology. The task is feasible but non-trivial, which is demonstrated by creating and comparing three alternative baseline systems. We believe that this corpus will be of interest to the researchers working in textual entailment and will stimulate new developments both in natural language processing in tutorial dialogue systems and textual entailment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/P2WYQILA/Dzikovska et al. - Towards Effective Tutorial Feedback for Explanatio.pdf}
}

@article{filigheraFoolingAutomaticShort2020,
  title = {Fooling {{Automatic Short Answer Grading Systems}}},
  author = {Filighera, Anna and Steuer, Tim and Rensing, Christoph},
  year = {2020},
  month = jun,
  journal = {Artificial Intelligence in Education},
  volume = {12163},
  pages = {177--190},
  doi = {10.1007/978-3-030-52237-7_15},
  abstract = {With the rising success of adversarial attacks on many NLP tasks, systems which actually operate in an adversarial scenario need to be reevaluated. For this purpose, we pose the following research question: How difficult is it to fool automatic short answer grading systems? In particular, we investigate the robustness of the state of the art automatic short answer grading system proposed by Sung et al. towards cheating in the form of universal adversarial trigger employment. These are short token sequences that can be prepended to students' answers in an exam to artificially improve their automatically assigned grade. Such triggers are especially critical as they can easily be used by anyone once they are found. In our experiments, we discovered triggers which allow students to pass exams with passing thresholds of \textbackslash textbackslash documentclass[12pt]\{minimal\} \textbackslash textbackslash usepackage\{amsmath\} \textbackslash textbackslash usepackage\{wasysym\} \textbackslash textbackslash usepackage\{amsfonts\} \textbackslash textbackslash usepackage\{amssymb\} \textbackslash textbackslash usepackage\{amsbsy\} \textbackslash textbackslash usepackage\{mathrsfs\} \textbackslash textbackslash usepackage\{upgreek\} \textbackslash textbackslash setlength\{\textbackslash textbackslash oddsidemargin\}\{-69pt\} \textbackslash textbackslash begin\{document\}\$\$50\textbackslash textbackslash\%\$\$\textbackslash textbackslash end\{document\}50\% without answering a single question correctly. Furthermore, we show that such triggers generalize across models and datasets in this scenario, nullifying the defense strategy of keeping grading models or data secret.},
  pmcid = {PMC7334174},
  pmid = {null}
}

@misc{HewlettFoundationAutomated,
  title = {The {{Hewlett Foundation}}: {{Automated Essay Scoring}}},
  shorttitle = {The {{Hewlett Foundation}}},
  abstract = {Develop an automated scoring algorithm for student-written essays.},
  howpublished = {https://kaggle.com/competitions/asap-aes},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/92ZZ4M23/asap-aes.html}
}

@misc{HewlettFoundationShort,
  title = {The {{Hewlett Foundation}}: {{Short Answer Scoring}}},
  shorttitle = {The {{Hewlett Foundation}}},
  abstract = {Develop a scoring algorithm for student-written short-answer responses.},
  howpublished = {https://kaggle.com/competitions/asap-sas},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/VGLFCWGV/asap-sas.html}
}

@inproceedings{johanberggrenRegressionClassificationAutomated2019,
  title = {Regression or {{Classification}}? {{Automated Essay Scoring}} for {{Norwegian}}},
  shorttitle = {Regression or {{Classification}}?},
  booktitle = {Proceedings of the {{Fourteenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  author = {Johan Berggren, Stig and Rama, Taraka and {\O}vrelid, Lilja},
  year = {2019},
  pages = {92--102},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4409},
  abstract = {In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.},
  langid = {english},
  keywords = {read}
}

@article{liuAutomatedEssayFeedback2017,
  title = {Automated {{Essay Feedback Generation}} and {{Its Impact}} on {{Revision}}},
  author = {Liu, Ming and Li, Yi and Xu, Weiwei and Liu, Li},
  year = {2017},
  month = oct,
  journal = {IEEE Transactions on Learning Technologies},
  volume = {10},
  number = {4},
  pages = {502--513},
  issn = {1939-1382},
  doi = {10.1109/TLT.2016.2612659},
  abstract = {Writing an essay is a very important skill for students to master, but a difficult task for them to overcome. It is particularly true for English as Second Language (ESL) students in China. It would be very useful if students could receive timely and effective feedback about their writing. Automatic essay feedback generation is a challenging task, which requires understanding the relationship between the text features of the essay and feedback. In this study, we first analyzed 1,290 teacher comments on their 327 Englishmajor students and annotated the feedback on seven aspects of writing, including the grammar, spelling, sentence diversity, structure, organization, supporting ideas, coherence, and conclusion, for each paper. Then, an automatic feedback classification experiment was conducted with the machine learning approach. Finally, we investigated the impact of the system generated-indirect corrective feedback (ICF) and human teachers' direct corrective feedback (DCF) in two English writing classes (N = 56 in ICF class; N = 54 in DCF class) at a key Chinese university through a web-based assignment management system. The study results indicated the feasibility of this approach that system generated ICF can be as useful as direct comments made by the teachers in terms of improving the quality of the content regarding to the structure, organization, supporting ideas, coherence, and conclusion, and encouraging students to spend more time on self-correction.},
  keywords = {Feature extraction,Grammar,natural language processing,Semantics,text analysis,Text mining,Writing,Writing feedback},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/KG5QD5JU/Liu et al. - 2017 - Automated Essay Feedback Generation and Its Impact.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/9XYCXSPT/7574351.html}
}

@misc{liuAutomatedEssayScoring2019,
  title = {Automated {{Essay Scoring}} Based on {{Two-Stage Learning}}},
  author = {Liu, Jiawei and Xu, Yang and Zhu, Yaguang},
  year = {2019},
  month = dec,
  number = {arXiv:1901.07744},
  eprint = {1901.07744},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Current state-of-the-art feature-engineered and endto-end Automated Essay Score (AES) methods are proven to be unable to detect adversarial samples, e.g. the essays composed of permuted sentences and the prompt-irrelevant essays. Focusing on the problem, we develop a Two-Stage Learning Framework (TSLF) which integrates the advantages of both feature-engineered and end-to-end AES methods. In experiments, we compare TSLF against a number of strong baselines, and the results demonstrate the effectiveness and robustness of our models. TSLF surpasses all the baselines on five-eighths of prompts and achieves new state-of-the-art average performance when without negative samples. After adding some adversarial eassys to the original datasets, TSLF outperforms the featuresengineered and end-to-end baselines to a great extent, and shows great robustness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/9ELLPRYQ/Liu et al. - 2019 - Automated Essay Scoring based on Two-Stage Learnin.pdf}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/DHAIZ5YF/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/IEFHA6Q5/1907.html}
}

@inproceedings{loukinaManyDimensionsAlgorithmic2019,
  title = {The Many Dimensions of Algorithmic Fairness in Educational Applications},
  booktitle = {Proceedings of the {{Fourteenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  author = {Loukina, Anastassia and Madnani, Nitin and Zechner, Klaus},
  year = {2019},
  pages = {1--10},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4401},
  abstract = {The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people's lives especially when deployed as part of highstakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers' native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/FRMG8XH5/Loukina et al. - 2019 - The many dimensions of algorithmic fairness in edu.pdf}
}

@article{ludwigAutomatedEssayScoring2021,
  title = {Automated {{Essay Scoring Using Transformer Models}}},
  author = {Ludwig, Sabrina and Mayer, Christian and Hansen, Christopher and Eilers, Kerstin and Brandt, Steffen},
  year = {2021},
  month = dec,
  journal = {Psych},
  volume = {3},
  number = {4},
  pages = {897--915},
  issn = {2624-8611},
  doi = {10.3390/psych3040056},
  abstract = {Automated essay scoring (AES) is gaining increasing attention in the education sector as it significantly reduces the burden of manual scoring and allows ad hoc feedback for learners. Natural language processing based on machine learning has been shown to be particularly suitable for text classification and AES. While many machine-learning approaches for AES still rely on a bag of words (BOW) approach, we consider a transformer-based approach in this paper, compare its performance to a logistic regression model based on the BOW approach, and discuss their differences. The analysis is based on 2088 email responses to a problem-solving task that were manually labeled in terms of politeness. Both transformer models considered in the analysis outperformed without any hyperparameter tuning of the regression-based model. We argue that, for AES tasks such as politeness classification, the transformer-based approach has significant advantages, while a BOW approach suffers from not taking word order into account and reducing the words to their stem. Further, we show how such models can help increase the accuracy of human raters, and we provide a detailed instruction on how to implement transformer-based models for one's own purposes.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/97FKI533/Ludwig et al. - 2021 - Automated Essay Scoring Using Transformer Models.pdf}
}

@article{muangkammuenMultitaskLearningAutomated,
  title = {Multi-Task {{Learning}} for {{Automated Essay Scoring}} with {{Sentiment Analysis}}},
  author = {Muangkammuen, Panitan and Fukumoto, Fumiyo},
  pages = {8},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/RH7VT5XZ/Muangkammuen and Fukumoto - Multi-task Learning for Automated Essay Scoring wi.pdf}
}

@article{nguyenNeuralNetworksAutomated2016,
  title = {Neural {{Networks}} for {{Automated Essay Grading}}},
  author = {Nguyen, Huyen and Dery, Lucio},
  year = {2016},
  pages = {11},
  abstract = {The biggest obstacle to choosing constructed-response assessments over traditional multiple-choice assessments is the large cost and effort required for scoring. This project is an attempt to use different neural network architectures to build an accurate automated essay grading system to solve this problem.},
  langid = {english},
  keywords = {read}
}

@inproceedings{pulmanAutomaticShortAnswer2005,
  title = {Automatic {{Short Answer Marking}}},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Building Educational Applications Using NLP}} - {{EdAppsNLP}} 05},
  author = {Pulman, Stephen G. and Sukkarieh, Jana Z.},
  year = {2005},
  pages = {9--16},
  publisher = {{Association for Computational Linguistics}},
  address = {{Ann Arbor, Michigan}},
  doi = {10.3115/1609829.1609831},
  langid = {english},
  keywords = {read}
}

@misc{rajapakseSimpleTransformers2021,
  title = {Simple {{Transformers}}},
  author = {Rajapakse, Thilina},
  year = {2021},
  month = aug,
  abstract = {Transformers for Classification, NER, QA, Language Modelling, Language Generation, T5, Multi-Modal, and Conversational AI},
  copyright = {Apache-2.0},
  keywords = {conversational-ai,named-entity-recognition,question-answering,text-classification,transformers}
}

@article{ramalingamAutomatedEssayGrading2018a,
  title = {Automated {{Essay Grading}} Using {{Machine Learning Algorithm}}},
  author = {Ramalingam, V. V. and Pandian, A and Chetry, Prateek and Nigam, Himanshu},
  year = {2018},
  month = apr,
  journal = {Journal of Physics: Conference Series},
  volume = {1000},
  pages = {012030},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1000/1/012030},
  abstract = {Essays are paramount for of assessing the academic excellence along with linking the different ideas with the ability to recall but are notably time consuming when they are assessed manually. Manual grading takes significant amount of evaluator's time and hence it is an expensive process. Automated grading if proven effective will not only reduce the time for assessment but comparing it with human scores will also make the score realistic. The project aims to develop an automated essay assessment system by use of machine learning techniques by classifying a corpus of textual entities into small number of discrete categories, corresponding to possible grades. Linear regression technique will be utilized for training the model along with making the use of various other classifications and clustering techniques. We intend to train classifiers on the training set, make it go through the downloaded dataset, and then measure performance our dataset by comparing the obtained values with the dataset values. We have implemented our model using java.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/69T6VB79/Ramalingam et al. - 2018 - Automated Essay Grading using Machine Learning Alg.pdf}
}

@article{rameshAutomatedEssayScoring2022,
  title = {An Automated Essay Scoring Systems: A Systematic Literature Review},
  shorttitle = {An Automated Essay Scoring Systems},
  author = {Ramesh, Dadi and Sanampudi, Suresh Kumar},
  year = {2022},
  month = mar,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {3},
  pages = {2495--2527},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-021-10068-2},
  abstract = {Assessment in the Education system plays a significant role in judging student performance. The present evaluation system is through human assessment. As the number of teachers' student ratio is gradually increasing, the manual evaluation process becomes complicated. The drawback of manual evaluation is that it is time-consuming, lacks reliability, and many more. This connection online examination system evolved as an alternative tool for pen and paper-based methods. Present Computer-based evaluation system works only for multiple-choice questions, but there is no proper evaluation system for grading essays and short answers. Many researchers are working on automated essay grading and short answer scoring for the last few decades, but assessing an essay by considering all parameters like the relevance of the content to the prompt, development of ideas, Cohesion, and Coherence is a big challenge till now. Few researchers focused on Content-based evaluation, while many of them addressed style-based assessment. This paper provides a systematic literature review on automated essay scoring systems. We studied the Artificial Intelligence and Machine Learning techniques used to evaluate automatic essay scoring and analyzed the limitations of the current studies and research trends. We observed that the essay evaluation is not done based on the relevance of the content and coherence.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/YEAIZF42/Ramesh and Sanampudi - 2022 - An automated essay scoring systems a systematic l.pdf}
}

@inproceedings{riordanHowAccountMispellings2019,
  title = {How to Account for Mispellings: {{Quantifying}} the Benefit of Character Representations in Neural Content Scoring Models},
  shorttitle = {How to Account for Mispellings},
  booktitle = {Proceedings of the {{Fourteenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  author = {Riordan, Brian and Flor, Michael and Pugh, Robert},
  year = {2019},
  pages = {116--126},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4411},
  abstract = {Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding character representations to word-based models and test these hypotheses on large-scale real-world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAPSAS short content scoring dataset.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/PM7S6ZCC/Riordan et al. - 2019 - How to account for mispellings Quantifying the be.pdf}
}

@inproceedings{riordanInvestigatingNeuralArchitectures2017,
  title = {Investigating Neural Architectures for Short Answer Scoring},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building}}           {{Educational Applications}}},
  author = {Riordan, Brian and Horbach, Andrea and Cahill, Aoife and Zesch, Torsten and Lee, Chong Min},
  year = {2017},
  pages = {159--168},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/W17-5017},
  abstract = {Neural approaches to automated essay scoring have recently shown state-of-theart performance. The automated essay scoring task typically involves a broad notion of writing quality that encompasses content, grammar, organization, and conventions. This differs from the short answer content scoring task, which focuses on content accuracy. The inputs to neural essay scoring models \textendash{} ngrams and embeddings \textendash{} are arguably well-suited to evaluate content in short answer scoring tasks. We investigate how several basic neural approaches similar to those used for automated essay scoring perform on short answer scoring. We show that neural architectures can outperform a strong nonneural baseline, but performance and optimal parameter settings vary across the more diverse types of prompts typical of short answer scoring.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/GFLLZ6PB/Riordan et al. - 2017 - Investigating neural architectures for short answe.pdf}
}

@inproceedings{sharmaAutomatedGradingHandwritten2018,
  title = {Automated {{Grading}} of {{Handwritten Essays}}},
  booktitle = {2018 16th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Sharma, Annapurna and Jayagopi, Dinesh Babu},
  year = {2018},
  month = aug,
  pages = {279--284},
  doi = {10.1109/ICFHR-2018.2018.00056},
  abstract = {Automatic grading of handwritten essays is vital in evaluating the performance of students in educational settings, particularly in situations where language experts are rare. We build a system capable of taking the input as handwritten essays in image format and outputs the grading on the scale of 0-5; 0 being the worst and 5 being the best. The overall system integrates Optical Handwriting Recognition (OHR) and Automated Essay Scoring (AES)/grading. The handwritten essay is transcribed using a network composed of Multi-Dimensional Long Short Term Memory (MDLSTM) and convolution layers. The loss function is Connectionist Temporal Classification (CTC). The AES model is a 2-layer artificial neural network with a feature set based on pretrained GloVe word vectors. The results of grading of essays are compared for transcriptions of essays received from OHR system and transcriptions of essays done manually. The mutual agreement between the two shows a Quadratic Weighted Kappa score of 0.88. The results indicate that though the current OHR systems have transcription errors but as a whole can perform well for an application like AES.},
  keywords = {Automated essay grading,Convolution,Feature extraction,GloVe-vector representation,Handwriting recognition,Hidden Markov models,MDLSTM,Measurement,Task analysis,Training},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/KY4MMXKK/8583774.html}
}

@inproceedings{sinhaAnswerEvaluationUsing2018,
  title = {Answer {{Evaluation Using Machine Learning}}},
  author = {Sinha, Prince and Kaul, Ayush and Bharadia, Sharad},
  year = {2018},
  pages = {7},
  abstract = {In this modern age, where the world moves towards automation so, there is a need for automation in answer evaluation system. Currently, the online answer evaluation is available for mcq based question, hence evaluation of the theory answer is hectic for the checker. Teacher manually checks the answer and allot the marks. The current system takes more manpower and time to evaluate the answer. In this journal an application based on the evaluation of answers using machine learning. The objective of the journal is to specially reduce the manpower and time consumption. Since in manual answer evaluation, the manpower and the time consumption is much more. Also, in the manual system, it may be possible that the marks given to two same answers are different. This application system provides an automatic evaluation of answer based on the keyword provided to the application in form of the input by the moderator which will provide equal distribution of marks and will reduce time and manpower.},
  langid = {english},
  keywords = {read},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/K37QBYH4/Answer_Evaluation_with_ML.pdf}
}

@inproceedings{sultanFastEasyShort2016,
  title = {Fast and {{Easy Short Answer Grading}} with {{High Accuracy}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Sultan, Md Arafat and Salazar, Cristobal and Sumner, Tamara},
  year = {2016},
  pages = {1070--1075},
  publisher = {{Association for Computational Linguistics}},
  address = {{San Diego, California}},
  doi = {10.18653/v1/N16-1123},
  abstract = {We present a fast, simple, and high-accuracy short answer grading system. Given a shortanswer question and its correct answer, key measures of the correctness of a student response can be derived from its semantic similarity with the correct answer. Our supervised model (1) utilizes recent advances in the identification of short-text similarity, and (2) augments text similarity features with key grading-specific constructs. We present experimental results where our model demonstrates top performance on multiple benchmarks.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/5H5JN6UB/Sultan et al. - 2016 - Fast and Easy Short Answer Grading with High Accur.pdf}
}

@inproceedings{taghipourNeuralApproachAutomated2016,
  title = {A {{Neural Approach}} to {{Automated Essay Scoring}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Taghipour, Kaveh and Ng, Hwee Tou},
  year = {2016},
  pages = {1882--1891},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1193},
  abstract = {Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6\% in terms of quadratic weighted Kappa, without requiring any feature engineering.},
  langid = {english}
}

@inproceedings{tashuSemanticBasedFeedbackRecommendation2020,
  title = {Semantic-{{Based Feedback Recommendation}} for {{Automatic Essay Evaluation}}},
  booktitle = {Intelligent {{Systems}} and {{Applications}}},
  author = {Tashu, Tsegaye Misikir and Horv{\'a}th, Tom{\'a}{\v s}},
  editor = {Bi, Yaxin and Bhatia, Rahul and Kapoor, Supriya},
  year = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {334--346},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-29513-4_24},
  abstract = {With the development of sophisticated e-learning platforms, educational recommender systems and automatic essay evaluation are becoming an important feature in e-learning systems. Most of the works in educational recommendation techniques are focused on recommending learning materials or learning activities to the learners. In this paper, we proposed and implemented a semantic-based feedback recommendation approach for automatic essay evaluation, which will allow assessors to interact with automatic essay evaluation systems, give feedback on learner's essay solution in the form of textual comments and provide recommendation to other similar essay solution based on the solution which the assessor has given textual feedback. To compute the semantic similarity and to provide feedback recommendation, we used neural word embedding and relaxed word mover's similarity. The proposed approach achieves high-performance accuracy, compared to the state-of-the-art methods, according to our experimental results.},
  isbn = {978-3-030-29513-4},
  langid = {english},
  keywords = {Educational recommender system,Feedback recommendation,Semantic similarity,Text similarity}
}

@misc{turcWellReadStudentsLearn2019,
  title = {Well-{{Read Students Learn Better}}: {{On}} the {{Importance}} of {{Pre-training Compact Models}}},
  shorttitle = {Well-{{Read Students Learn Better}}},
  author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = sep,
  number = {arXiv:1908.08962},
  eprint = {1908.08962},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/T726ADVX/Turc et al. - 2019 - Well-Read Students Learn Better On the Importance.pdf;/home/oscar/snap/zotero-snap/common/Zotero/storage/YBQADILI/1908.html}
}

@article{utoReviewDeepneuralAutomated2021,
  title = {A Review of Deep-Neural Automated Essay Scoring Models},
  author = {Uto, Masaki},
  year = {2021},
  month = jul,
  journal = {Behaviormetrika},
  volume = {48},
  number = {2},
  pages = {459--484},
  issn = {0385-7417, 1349-6964},
  doi = {10.1007/s41237-021-00142-y},
  abstract = {Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to grading by humans. Although traditional AES models typically rely on manually designed features, deep neural network (DNN)-based AES models that obviate the need for feature engineering have recently attracted increased attention. Various DNN-AES models with different characteristics have been proposed over the past few years. To our knowledge, however, no study has provided a comprehensive review of DNN-AES models while introducing each model in detail. Therefore, this review presents a comprehensive survey of DNNAES models, describing the main idea and detailed architecture of each model. We classify the AES task into four types and introduce existing DNN-AES models according to this classification.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/UJL9F54S/Uto - 2021 - A review of deep-neural automated essay scoring mo.pdf}
}

@inproceedings{utoRobustNeuralAutomated2020,
  title = {Robust {{Neural Automated Essay Scoring Using Item Response Theory}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Uto, Masaki and Okano, Masashi},
  editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Mill{\'a}n, Eva},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {549--561},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-52237-7_44},
  abstract = {Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to human grading. Conventional AES methods typically rely on manually tuned features, which are laborious to effectively develop. To obviate the need for feature engineering, many deep neural network (DNN)-based AES models have been proposed and have achieved state-of-the-art accuracy. DNN-AES models require training on a large dataset of graded essays. However, assigned grades in such datasets are known to be strongly biased due to effects of rater bias when grading is conducted by assigning a few raters in a rater set to each essay. Performance of DNN models rapidly drops when such biased data are used for model training. In the fields of educational and psychological measurement, item response theory (IRT) models that can estimate essay scores while considering effects of rater characteristics have recently been proposed. This study therefore proposes a new DNN-AES framework that integrates IRT models to deal with rater bias within training data. To our knowledge, this is a first attempt at addressing rating bias effects in training data, which is a crucial but overlooked problem.},
  isbn = {978-3-030-52237-7},
  langid = {english},
  keywords = {Automated essay scoring,Deep neural networks,Item response theory,Rater bias},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/DRNC9F3W/Uto and Okano - 2020 - Robust Neural Automated Essay Scoring Using Item R.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
  year = {2017},
  pages = {11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  keywords = {technology}
}

@inproceedings{wangAutomaticEssayScoring2018,
  title = {Automatic {{Essay Scoring Incorporating Rating Schema}} via {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wang, Yucheng and Wei, Zhongyu and Zhou, Yaqian and Huang, Xuanjing},
  year = {2018},
  pages = {791--797},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1090},
  abstract = {Automatic essay scoring (AES) is the task of assigning grades to essays without human interference. Existing systems for AES are typically trained to predict the score of each single essay at a time without considering the rating schema. In order to address this issue, we propose a reinforcement learning framework for essay scoring that incorporates quadratic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our framework.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/7U5AYUDC/Wang et al. - 2018 - Automatic Essay Scoring Incorporating Rating Schem.pdf}
}

@misc{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = oct,
  pages = {38--45},
  abstract = {ðŸ¤— Transformers: State-of-the-art Natural Language Processing for Pytorch, TensorFlow, and JAX.},
  copyright = {Apache-2.0}
}

@inproceedings{zhuAutomatedEssayScoring2020,
  title = {Automated {{Essay Scoring System}} Using {{Multi-Model Machine Learning}}},
  booktitle = {Computer {{Science}} \& {{Information Technology}} ({{CS}} \& {{IT}})},
  author = {Zhu, Wilson and Sun, Yu},
  year = {2020},
  month = oct,
  pages = {109--117},
  publisher = {{AIRCC Publishing Corporation}},
  doi = {10.5121/csit.2020.101211},
  abstract = {Standardized testing such as the SAT often requires students to write essays and hires a large number of graders to evaluate these essays which can be time and cost consuming. Using natural language processing tools such as Global Vectors for word representation (GloVe), and various types of neural networks designed for picture classification, we developed an automatic grading system that is more time- and cost-efficient compared to human graders. We applied our application to a set of manually graded essays provided by a previous competition on Kaggle in 2012 on automated essay grading and conducted a qualitative evaluation of the approach. The result shows that the program is able to correctly score most of the essay and give an evaluation close to that of a human grader on the rest. The system proves itself to be effective in evaluating various essay prompts and capable of real-life application such as assisting another grader or even used as a standalone grader.},
  langid = {english},
  file = {/home/oscar/snap/zotero-snap/common/Zotero/storage/4GIDI59C/Zhu and Sun - 2020 - Automated Essay Scoring System using Multi-Model M.pdf}
}


